{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c515f1-3338-45fd-b63a-b5e2dc184a61",
   "metadata": {},
   "source": [
    "### Model-Weighting-Toolbox\n",
    "\n",
    "Tested on Python 3.6.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c59a2-686c-4090-89ca-c4999341ae96",
   "metadata": {},
   "source": [
    "### BMA Function Definitions\n",
    "\n",
    "* The key idea with BMA is to combine different models to improve prediction performance.\n",
    "* BMA works by sampling different weights and then evaulating the performance of the combined model using those weights.\n",
    "* The key metric for evaluation is RMSE, which is defined as $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2}$$\n",
    "    * Where $y_i$ is the $i$th observation and $$\\hat{y}_i=\\sum_{m=1}^M w_m\\hat{y}_{im}$$ \n",
    "    * is the weighted average of the models' prediction values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* By repeatedly sampling weights, one can obtain an estimate of the optimal weights according to RMSE\n",
    "* The best set of weights ($x_{\\text{best}}$) as well as the posterior set of weights ($x_{\\text{posterior}}$) are provided to the user as: \n",
    "\n",
    "$$ (\\text{weights}_{\\text{BMA,optimal}},\\,\\text{weights}_{\\text{BMA,posterior}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ca424-095b-4755-a7bb-45099f688732",
   "metadata": {},
   "source": [
    "### Calculating independence for BMA (for lines #51-53)\n",
    "\n",
    "More recently, weighting based on model independence has been an additional criterion to consider alongside model skill. This consideration of model independence has emerged due to models having common bases of model structure, parameterizations, and associated programming code, all of which can result in a lack of independence between climate models. Here, independence information is estimated in the post-processing of the model weights, and we determine model independence information using the posterior distribution of the BMA weights. To quantify independence here, we take the sum of the correlation matrix of the posterior samples and subtract 1 from this. This results in a negative independence score for models that have independent posterior samples and a positive independence score for models that have dependent posterior samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7de06-0cfa-40f7-9f2c-3f191dd41b6d",
   "metadata": {},
   "source": [
    "### AIC Definition\n",
    "\n",
    "AIC can also be used to weight different models. AIC captures some notion of model fit, so it makes sense to use this information to weight higher the models that fit the data the best. The formula for AIC is $$\\text{AIC} = -2\\hat{L} + 2p$$ where $\\hat{L}$ is the the estimated likelihood from the model and $p$ is the number of parameters in the model.\n",
    "\n",
    "When used for weighting, a typical approach is to use the weight $$\\text{weights}_{\\text{AIC}} = \\frac{e^{-\\frac{1}{2}\\text{AIC}_m}}{\\sum_{m=1}^M e^{-\\frac{1}{2}\\text{AIC}_m}}$$\n",
    "\n",
    "for the $m$th model\n",
    "$$\\hat{y}_i=\\sum_{m=1}^M w_m\\hat{y}_{im}$$.\n",
    "\n",
    "The difference between this approach (AIC) and BMA is that BMA samples and tests the performance of different weights to see which weights are optimal. Whereas AIC simply uses the fit of each model to determine the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d689de-b2cd-4df5-b244-be3c50236931",
   "metadata": {},
   "source": [
    "### The Sanderson Approach for both skill and independence\n",
    "\n",
    "* Models differ in their skill to simulate observations, and there is an inherent lack of independence between models. \n",
    "    * There are weighting strategies in the literature for use with climate model ensembles, which consider both model performance skill as well as the interdependency of models that arises from common development and calibration practices of the models.\n",
    "    * These strategies are used to estimate a model average from several ensemble members that were simulated from dependent models of varying skill.\n",
    "    * In this context, we use the RMSE as a representation of skill, however users can create and use different options for this metric. \n",
    "    * This score is used as an input to develop our skill metric weights. We follow a similar recipe as Sanderson et al. (2015, 2017) and estimate model weights using the skill metrics with:\n",
    "    \n",
    "    $$w_{m,\\text{Skill}(i)} = Ae^{- \\big (\\frac{\\delta_{i,\\text{(obs)}}}{D_q} \\big )^2}$$\n",
    "\n",
    "* Where $A$ is a normalizing constant such that the sum of all the weights is equal to 1, $\\delta_{i,\\text{(obs)}}$ is the “Median Skill” score for each model, and $D_q$ is the radius of model quality, which determines the degree to which models with poor skill should be down‐weighted. A very small value of $D_q$ will allocate a large fraction of weight to the single best‐performing model. Equally, as $D_q$ approaches infinity, the model average will include more information from the non‐skill weighted models. For our study, we use a value of $D_q = 0.9$\n",
    "\n",
    "* The independence scores for each model can be calculated using the intermodel distances, which are a function of the RMSE between each of the models. The intermodel distance matrix is constructed with the RMSE values, and the similarity scores, $S(\\delta_{i,j})$, are then calculated for each pair of models $i$ and $j$ using this distance matrix. This score is calculated as follows:\n",
    "\n",
    "$$S(\\delta_{i,j}) = e^{-\\big ( \\frac{\\delta_{i,j}}{D_u} \\big )^2}$$\n",
    "\n",
    "* Where $\\delta_{i,j}$ are the RMSE values from the intermodel distance matrix and $D_u$ is the radius of similarity (Sanderson et al., 2015), which is a free parameter that determines the distance scale over which models should be considered similar and thus down‐weighted for co‐dependence. We choose a value of $D_u = 0.5$, which is shown in Sanderson et al. (2015), to be an appropriate value. In theory, two identical models will produce a similarity score of $S(\\delta_{i,j}) = 1$, and this score approaches 0 as the distance between the models grows infinitely.\n",
    "\n",
    "* The independence scores, $w_u(i)$, are calculated using the similarity scores for each model in the following manner:\n",
    "\n",
    "$$w_u(i) = \bigg { 1 + \sum_{j\neq i}^n S(\delta_{i,j}) \bigg }^{-1}$$,
    "\n",
    "* Where $n$ is the total number of models. Finally the equation used to combine the uniqueness and skill information to create the Sanderson set of model weights is\n",
    "\n",
    "$$\\text{weights}_{\\text{Sanderson}}(i) = Aw_{m,\\text{Skill}(i)}w_u(i)$$\n",
    "\n",
    "* Where $A$ is a normalizing constant such that the sum of all the weights is equal to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
